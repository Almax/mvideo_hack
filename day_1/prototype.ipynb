{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:28: UserWarning: [Errno 2] No such file or directory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n",
      "/usr/lib/python2.7/dist-packages/joblib/_multiprocessing_helpers.py:29: UserWarning: [Errno 2] No such file or directory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nile.api.v1 import clusters\n",
    "from nile.api.v1 import aggregators as na\n",
    "from qb2.api.v1 import filters as qf\n",
    "from nile.api.v1 import filters as nf\n",
    "from nile.api.v1 import extractors as ne\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing import sequence\n",
    "import time\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib\n",
    "import pickle\n",
    "import gc\n",
    "from keras.models import load_model\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm import tqdm\n",
    "import pymorphy2\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 30000\n",
    "n_components = 10\n",
    "n_top_words = 40\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stopWords = stopwords.words('russian')\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopWords)#,input=\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_exclude = pickle.load(open('features_not_relevant', 'rb'))\n",
    "features_to_include = pickle.load(open('features_relevant', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация и стоп символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "exclude = set(punctuation + '0123456789'+u'–—'+u'«»')\n",
    "import util_david"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma_dict = {}\n",
    "def lemmatizer(word):\n",
    "    if word in lemma_dict:\n",
    "        return lemma_dict[word]\n",
    "    else:\n",
    "        normal_form = morph.parse(word)[0].normal_form\n",
    "        lemma_dict[word] = normal_form\n",
    "        return normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных  - dataset1.csv =  это переименованный dataset1_ver2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1  = pd.read_csv('data/dataset1.csv') #\n",
    "x2  = pd.read_csv('data/dataset2.csv')\n",
    "x3  = pd.read_csv('data/views.csv',sep = ';')\n",
    "x4  = pd.read_csv('data/compare_v3.csv',sep = ';')\n",
    "dist_categories_id = list(x1.CATEGORY_ID.unique())\n",
    "dist_categories_names = list(x1.CATEGORY_NAME.unique())\n",
    "mappings = x1[[\"CATEGORY_ID\",'CATEGORY_NAME']].drop_duplicates()\n",
    "mappings['Number_of_reviews'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/780 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:03<00:00, 241.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not relevant excluded\n",
      "        weight          word  exclude\n",
      "1942  0.142212         тихий        0\n",
      "1105  0.084534    олеофобный        0\n",
      "382   0.080078        деньга        0\n",
      "1632  0.069275        ремонт        0\n",
      "1563  0.044037          пыль        0\n",
      "845   0.036316      мерцание        0\n",
      "73    0.032166    аккуратный        0\n",
      "1309  0.030060     подсветка        0\n",
      "1255  0.028671       пластик        0\n",
      "1947  0.028534         толща        0\n",
      "821   0.025864       матовый        0\n",
      "654   0.024490        камера        0\n",
      "1668  0.022629       самсунг        0\n",
      "2127  0.021927          цена        0\n",
      "1206  0.017822         ощупь        0\n",
      "725   0.016220        корпус        0\n",
      "820   0.015686      материал        0\n",
      "556   0.015350        звонок        0\n",
      "1868  0.014801        стекло        0\n",
      "1732  0.014366       скачать        0\n",
      "1663  0.014351      садиться        0\n",
      "664   0.013771  качественный        0\n",
      "532   0.013077         заряд        0\n",
      "1594  0.011810        размер        0\n",
      "1267  0.011246       площадь        0\n",
      "834   0.010674       мелодия        0\n",
      "540   0.009949      застёжка        0\n",
      "1128  0.009911            ос        0\n",
      "1586  0.009636        развод        0\n",
      "1767  0.008667       слышный        0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "only relevant included\n",
      "        weight          word  exclude\n",
      "1942  0.160400         тихий        0\n",
      "1105  0.103760    олеофобный        0\n",
      "382   0.091980        деньга        0\n",
      "1632  0.081970        ремонт        0\n",
      "1309  0.047729     подсветка        0\n",
      "845   0.044525      мерцание        0\n",
      "73    0.042664    аккуратный        0\n",
      "1947  0.035828         толща        0\n",
      "654   0.034912        камера        0\n",
      "1563  0.033966          пыль        0\n",
      "2127  0.032104          цена        0\n",
      "664   0.031952  качественный        0\n",
      "821   0.031525       матовый        0\n",
      "1255  0.028580       пластик        0\n",
      "556   0.025131        звонок        0\n",
      "2009  0.024704       удобный        0\n",
      "339   0.020355           год        0\n",
      "725   0.019348        корпус        0\n",
      "1868  0.017624        стекло        0\n",
      "1267  0.017410       площадь        0\n",
      "834   0.012733       мелодия        0\n",
      "1586  0.011490        развод        0\n",
      "1767  0.010338       слышный        0\n",
      "395   0.008026        дизайн        0\n",
      "128   0.007053       большой        0\n",
      "665   0.005619      качество        0\n",
      "1020  0.005306         новый        0\n",
      "149   0.004524        быстро        0\n",
      "688   0.002905        кнопка        0\n",
      "1737  0.001913     скользкий        0\n"
     ]
    }
   ],
   "source": [
    "reserv = 'iPhone 6'\n",
    "name = raw_input() or reserv\n",
    "c = 0\n",
    "iphone_6_review = []\n",
    "for i in xrange(x1.shape[0]):\n",
    "    try:\n",
    "        if name in x1.NAME.values[i]:\n",
    "            c+=1\n",
    "            iphone_6_review.append(i)\n",
    "            #break\n",
    "    except:\n",
    "        continue\n",
    "print(c)\n",
    "iphone6 = x1.ix[iphone_6_review,:]\n",
    "\n",
    "res_iphone_6 = []\n",
    "for person in tqdm(iphone6.TEXT.values,miniters=10000):\n",
    "    new_string = \"\"\n",
    "    for sentence in person.decode('utf-8').split():\n",
    "        for char in exclude:\n",
    "            sentence = sentence.replace(char,\"\")\n",
    "        new_string =new_string +' '+ sentence\n",
    "    new_string_2 = \"\"\n",
    "    for word in new_string.split():\n",
    "        new_string_2 = new_string_2 + \" \"+ lemmatizer(word)\n",
    "        \n",
    "\n",
    "    res_iphone_6.append(new_string_2)\n",
    "gc.collect()\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(res_iphone_6)\n",
    "y = iphone6.RATING\n",
    "model.fit(tf,y)\n",
    "\n",
    "fi_ip = model.feature_importances_\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "fi_ip = pd.DataFrame(fi_ip)\n",
    "fi_ip['word'] = tf_feature_names\n",
    "fi_ip.rename(columns={0:'weight'},inplace=True)\n",
    "fi_ip.sort(columns='weight',ascending=False,inplace=True)\n",
    "fi_ip = fi_ip[fi_ip.weight>0]\n",
    "fi_ip['exclude'] = 0\n",
    "\n",
    "for n,word in enumerate(fi_ip.word.values):\n",
    "    if word in features_to_exclude:\n",
    "        fi_ip.exclude.values[n] = 1\n",
    "fi_ip_adjusted = fi_ip[fi_ip.exclude==0]\n",
    "\n",
    "fi_ip_adjusted.weight = fi_ip_adjusted.weight/np.sum(fi_ip_adjusted.weight)\n",
    "fi_ip_adjusted.weight = np.float16(fi_ip_adjusted.weight)\n",
    "\n",
    "print ('not relevant excluded')\n",
    "print(fi_ip_adjusted[0:30])\n",
    "print(100*'-')\n",
    "tf = tf_vectorizer.fit_transform(res_iphone_6)\n",
    "y = iphone6.RATING\n",
    "model.fit(tf,y)\n",
    "\n",
    "fi_ip = model.feature_importances_\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "fi_ip = pd.DataFrame(fi_ip)\n",
    "fi_ip['word'] = tf_feature_names\n",
    "fi_ip.rename(columns={0:'weight'},inplace=True)\n",
    "fi_ip.sort(columns='weight',ascending=False,inplace=True)\n",
    "fi_ip = fi_ip[fi_ip.weight>0]\n",
    "fi_ip['exclude'] = 1\n",
    "\n",
    "for n,word in enumerate(fi_ip.word.values):\n",
    "    if word in features_to_include:\n",
    "        fi_ip.exclude.values[n] = 0\n",
    "fi_ip_adjusted = fi_ip[fi_ip.exclude==0]\n",
    "\n",
    "fi_ip_adjusted.weight = fi_ip_adjusted.weight/np.sum(fi_ip_adjusted.weight)\n",
    "fi_ip_adjusted.weight = np.float16(fi_ip_adjusted.weight)\n",
    "print ('only relevant included')\n",
    "\n",
    "print(fi_ip_adjusted[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
